from src.web_search import bing_web_search
from src.chatgpt import openai_generate_response
from src.crawlers.crawler_factory import CrawlerFactory
import asyncio
import os
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®è¨­å®š
CRAWLER_TYPE = os.getenv("CRAWLER_TYPE", "scrapy")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯scrapy
CRAWLER_CONFIG = {
    "user_agent": "ã‚«ã‚¹ã‚¿ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
    "respect_robots": True,
    "delay": 2
}

async def fetch_webpage_content(url: str) -> str:
    """ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    crawler = CrawlerFactory.create_crawler(CRAWLER_TYPE, CRAWLER_CONFIG)
    try:
        result = await crawler.fetch_content(url)
        return result.content if not result.error else ""
    finally:
        crawler.cleanup()

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    user_query = input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    print("\nã‚¦ã‚§ãƒ–æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™...\n")

    try:
        # Azure Web Search APIã‚’å‘¼ã³å‡ºã™
        search_results = bing_web_search(user_query)
        web_pages = search_results.get("webPages", {}).get("value", [])
        
        if not web_pages:
            print("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # æ¤œç´¢çµæœã‹ã‚‰URLã‚’æŠ½å‡ºã—ã€ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—
        detailed_summaries = []
        for i, page in enumerate(web_pages, 1):
            url = page.get("url")
            print(f"\n[{i}/{len(web_pages)}] å–å¾—ä¸­: {url}")
            
            content = await fetch_webpage_content(url)
            detailed_summaries.append({
                "title": page.get("name"),
                "url": url,
                "snippet": page.get("snippet"),
                "content": content[:1000] if content else "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
            })

        # OpenAIã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        prompt = (
            f"ä»¥ä¸‹ã¯ '{user_query}' ã«é–¢ã™ã‚‹ã‚¦ã‚§ãƒ–æ¤œç´¢ã®çµæœã§ã™ã€‚å„ã‚½ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ç·åˆçš„ã«åˆ†æã—ã€"
            f"çŸ›ç›¾ã™ã‚‹æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’æŒ‡æ‘˜ã—ã€æƒ…å ±ã®ä¿¡é ¼æ€§ã‚‚è€ƒæ…®ã—ã¦å›ç­”ã—ã¦ãã ã•ã„:\n\n"
        )

        for summary in detailed_summaries:
            prompt += (
                f"ã‚½ãƒ¼ã‚¹: {summary['url']}\n"
                f"ã‚¿ã‚¤ãƒˆãƒ«: {summary['title']}\n"
                f"æ¦‚è¦: {summary['snippet']}\n"
                f"å†…å®¹: {summary['content']}\n\n"
            )

        print("\nOpenAI GPTãƒ¢ãƒ‡ãƒ«ã«å¿œç­”ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¦ã„ã¾ã™...\n")

        # OpenAI GPT APIã‚’å‘¼ã³å‡ºã™
        ai_response = openai_generate_response(prompt)
        
        # å¿œç­”ã‚’è¡¨ç¤º
        answer = ai_response["choices"][0]["message"]["content"]
        print("\nğŸ’¡ **AIå¿œç­”**\n")
        print(answer)

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())